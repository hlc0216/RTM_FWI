/* Prestack time migration (2-D/3-D) CUDA kernel. */
/*
Copyright (C) 2010 University of Texas at Austin

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
*/


#include "su.h"
#include "segy.h"
#include "header.h"
#include "stdio.h"
#include "stdlib.h"
#include "math.h"
#include <string.h>
#include <math.h>
#include <time.h>
#include <cuda_runtime.h>





#define BLOCK_SIZE 128
#define MAX_TEXLEN 1 << 27
#define MAX_TEXSIDE 32768
#define MAX_GRID_SIZE 65535

__constant__ int c_obn; /* Number of input traces per each kernel call */
__constant__ int c_nt; /* Number of samples in each input trace */
__constant__ float c_ot; /* Input data t0 */
__constant__ float c_dt; /* Input data sample rate */
__constant__ float c_idt; /* Inverse of input data sample rate */
__constant__ int c_ont; /* Number of samples in each output trace */
__constant__ float c_oot; /* Output data t0 */
__constant__ float c_odt; /* Output data sample rate */

__constant__ float c_oox; /* First x in output */
__constant__ float c_odx; /* Output sample rate in x */
__constant__ int c_onx; /* Number of samples x in output */
__constant__ float c_ooy; /* First y in output */
__constant__ float c_ody; /* Output sample rate in y */
__constant__ int c_ony; /* Number of samples y in output */

__constant__ int c_aox; /* Apperture first indices in x */
__constant__ int c_aoy; /* Apperture first indices in y */
__constant__ int c_anx; /* Apperture number of indices in x */
__constant__ int c_any; /* Apperture number of indices in y */

__constant__ int c_ibn; /* Number of calculated distances per trace */

__constant__ float c_trf; /* Trace factor for antialiasing */
__constant__ float c_trm; /* Maximum half-length of the filter */
__constant__ float c_maxnt; /* Maximum input sample index, usually nt - 1 */

							/* Array of shot coordinates for input traces */
texture<float2, 1, cudaReadModeElementType> t_sxy;
/* Array of receiver coordinates for input traces */
texture<float2, 1, cudaReadModeElementType> t_gxy;
/* Array of surface vectors to source and receiver */
texture<float4, 1, cudaReadModeElementType> t_ixy;
/* Input data vector as a texture */
texture<float, 2, cudaReadModeElementType> t_i;
/* Aperture indices */
texture<uint, 1, cudaReadModeElementType> t_ap;

/****************************************************************
*
* Differentiation (backward second order) kernel
*
****************************************************************/

/*
t[gridDim.x*c_nt] - output vector of differentiated traces,
also input
*/

void sf_check_gpu_error(const char *msg)
/*< check GPU errors >*/
{
	cudaError_t err = cudaGetLastError();
	if (cudaSuccess != err) {
		warn("Cuda error: %s: %s", msg, cudaGetErrorString(err));
		exit(0);
	}
}


__global__ void sf_gpu_ktmig_sbdiff(float *t, const unsigned int n) {
	__shared__ float buffer[BLOCK_SIZE + 2];
	unsigned int tidx = blockIdx.x*c_nt + threadIdx.x;
	float val0 = 0, val1 = 0, val2 = 0;
	int i;

	buffer[threadIdx.x] = 0;
	__syncthreads();
	for (i = 0; i < n; i++) {
		val0 = t[tidx];
		/* Value at t0 */
		buffer[2 + threadIdx.x] = val0;
		__syncthreads();
		/* Value at t-1 */
		val1 = buffer[1 + threadIdx.x];
		/* Value at t-2 */
		val2 = buffer[threadIdx.x];
		/* Derivative */
		t[tidx] = 0.5f*c_idt*(3.0f*val0 - 4.0f*val1 + val2);
		__syncthreads();
		/* Shift everything down */
		buffer[(threadIdx.x + 2) % blockDim.x] = val0;
		__syncthreads();
		tidx += blockDim.x;
	}
}

/****************************************************************
*
* Causal integration kernel
*
****************************************************************/

/*
t[gridDim.x*c_nt] - output vector of causally integrated traces,
also input
*/
__global__ void sf_gpu_ktmig_cint(float *t, const unsigned int n) {
	__shared__ float buffer[BLOCK_SIZE * 2];
	unsigned int tidx = blockIdx.x*c_nt + threadIdx.x;
	float val1 = 0, val2 = 0;
	int i, j;

	val1 = t[tidx];
	buffer[threadIdx.x] = val1;
	for (i = 0; i < n; i++) {
		/* Read next portion to the buffer */
		if (i != (n - 1)) {
			val2 = t[tidx + blockDim.x];
			buffer[blockDim.x + threadIdx.x] = val2;
			__syncthreads();
		}
		/* Integrate causally the current portion */
		for (j = 1; j <= blockDim.x; j++) {
			buffer[threadIdx.x + j] += val1;
			__syncthreads();
		}
		t[tidx] = buffer[threadIdx.x];
		/* Shift buffer down */
		if (i != (n - 1)) {
			buffer[threadIdx.x] = buffer[blockDim.x + threadIdx.x];
			__syncthreads();
		}
		tidx += blockDim.x;
		val1 += val2;
	}
}

/****************************************************************
*
* Anti-causal integration kernel
*
****************************************************************/

/*
t[gridDim.x*c_nt] - output vector of anti-causally integrated traces,
also input
*/
__global__ void sf_gpu_ktmig_acint(float *t, const unsigned int n) {
	__shared__ float buffer[BLOCK_SIZE * 2];
	/* Base index in the output vector */
	unsigned int tidx = (blockIdx.x + 1)*c_nt - blockDim.x + threadIdx.x;
	/* Base index in the local buffer */
	unsigned int ltidx = blockDim.x - threadIdx.x - 1;
	float val1 = 0, val2 = 0;
	int i, j;

	val1 = t[tidx];
	buffer[ltidx] = val1;
	for (i = 0; i < n; i++) {
		/* Read next portion to the buffer */
		if (i != (n - 1)) {
			val2 = t[tidx - blockDim.x];
			buffer[blockDim.x + ltidx] = val2;
			__syncthreads();
		}
		/* Integrate anti-causally the current portion */
		for (j = 1; j <= blockDim.x; j++) {
			buffer[ltidx + j] += val1;
			__syncthreads();
		}
		t[tidx] = buffer[ltidx];
		/* Shift buffer down */
		if (i != (n - 1)) {
			buffer[ltidx] = buffer[blockDim.x + ltidx];
			__syncthreads();
		}
		tidx -= blockDim.x;
		val1 += val2;
	}
}

/****************************************************************
*
* Source/receiver-to-image point distance calculation kernel
*
****************************************************************/

/*
ixy[onx*ony*c_obn] - surface vectors to source and receiver for each image location
*/
__global__ void sf_gpu_ktmig_ixy(float4 *ixy) {
	int i;
	float x, y; /* Image locations */
	float2 xy; /* Source/receiver locations */
	float4 dist;

	/* Image coordinates */
	x = c_oox + (c_aox + (blockIdx.x*blockDim.x + threadIdx.x) % c_anx)*c_odx;
	y = c_ooy + (c_aoy + (blockIdx.x*blockDim.x + threadIdx.x) / c_anx)*c_ody;
	for (i = 0; i < c_obn; i++) {
		/* Source surface vector components */
		xy = tex1Dfetch(t_sxy, i);
		dist.x = x - xy.x;
		dist.y = y - xy.y;
		/* Receiver surface vector components */
		xy = tex1Dfetch(t_gxy, i);
		dist.z = x - xy.x;
		dist.w = y - xy.y;
		ixy[(i*gridDim.x + blockIdx.x)*blockDim.x + threadIdx.x] = dist;
		__syncthreads();
	}
}

/****************************************************************
*
* PSTM kernel with anti-aliasing after Lumley-Claerbout
*
****************************************************************/

/*
vrms[onx*ony*ont]  - RMS velocity vector,
image[onx*ony*ont] - output image vector,
lshift             - shift from the beginning of the array of aperture indices.
*/
__global__ void sf_gpu_ktmig_kernel(float *vrms, float *image,
	const unsigned int lshift) {
	/* Index within the output trace */
	const unsigned int tidx = blockDim.x*blockIdx.x + threadIdx.x;
	/* Index within the aperture block */
	const unsigned int bidx = tex1Dfetch(t_ap, blockIdx.y + lshift);
	/* Index within the output vector:
	(Position in y within the grid + position in x within the grid)
	*(number of time samples per output trace)
	+ shift in trace */
	const unsigned int oidx = ((c_aoy + bidx / c_anx)*c_onx + c_aox + bidx%c_anx)*c_ont + tidx;
	/* RMS velocity at image location */
	const float v = vrms[oidx];
	/* Slowness at image location */
	const float inv = 1.0f / v;
	const float inv2trf = c_trf*inv*inv;
	/* Pseudodepth^2 at image location */
	//  const float depth2 = powf (0.5f*v*(c_oot + tidx*c_odt), 2.0f);
	const float depth2 = 0.25f*v*v*(c_oot + tidx*c_odt)*(c_oot + tidx*c_odt);
	int i;
	float j, k;
	float img = 0.0f, scale, smp;
	float2 vec1;
	float4 vec2;

	/* Loop over input traces */
	for (i = 0; i < c_obn; i++) {
		vec2 = tex1Dfetch(t_ixy, i*c_ibn + bidx);
		/* vec1.x - squared distance to source from the image point on the surface,
		vec1.y - squared distance to receiver from the image point on the surface */
		vec1.x = vec2.x*vec2.x + vec2.y*vec2.y;
		vec1.y = vec2.z*vec2.z + vec2.w*vec2.w;
		/* Time from source to image point in pseudodepth */
		vec1.x = sqrtf(vec1.x + depth2)*inv;
		/* Time from receiver to image point in pseudodepth */
		vec1.y = sqrtf(vec1.y + depth2)*inv;
		/* double root square time = time to source + time to receiver */
		j = (vec1.x + vec1.y - c_ot)*c_idt; /* Input sample index */
											/* (distance to source.x)/(time to source) + (distance to receiver.x)/(time to receiver) */
		vec2.x = vec2.x / vec1.x + vec2.z / vec1.y;
		/* (distance to source.y)/(time to source) + (distance to receiver.y)/(time to receiver) */
		vec2.y = vec2.y / vec1.x + vec2.w / vec1.y;
		/* Filter length */
		k = inv2trf*sqrtf(vec2.x*vec2.x + vec2.y*vec2.y);
		/* Truncate filter */
		k = fminf(k, c_trm);
		/* If any of the three points is out of range - zero everything out */
		if ((j - k - 1.0f) >= 0.0f && (j + k + 1.0f) <= c_maxnt) {
			/* Scaling factor */
			scale = 1.0f / (1.0f + k);
			scale *= scale;
			/* Collect samples */
			smp = 2.0f*tex2D(t_i, j + 0.5f, i + 0.5f)
				- tex2D(t_i, j - k - 0.5f, i + 0.5f)
				- tex2D(t_i, j + k + 1.5f, i + 0.5f);
			/* Contribute to the image point */
			img += scale*smp;
		}
	}

	image[oidx] += img;
}

/****************************************************************
*
* PSTM kernel without anti-aliasing (simplified version from above)
*
****************************************************************/

/*
vrms[onx*ony*ont]  - RMS velocity vector,
image[onx*ony*ont] - output image vector,
lshift             - shift from the beginning of the array of aperture indices
*/
__global__ void sf_gpu_ktmig_noaa_kernel(float *vrms, float *image,
	const unsigned int lshift) {
	/* Index within the output trace */
	const unsigned int tidx = blockDim.x*blockIdx.x + threadIdx.x;
	/* Index within the aperture block */
	const unsigned int bidx = tex1Dfetch(t_ap, blockIdx.y + lshift);
	/* Index within the output vector:
	(Position in y within the grid + position in x within the grid)
	*(number of time samples per output trace)
	+ shift in trace */
	const unsigned int oidx = ((c_aoy + bidx / c_anx)*c_onx + c_aox + bidx%c_anx)*c_ont + tidx;
	/* RMS velocity at image location */
	const float v = vrms[oidx];
	/* Slowness at image location */
	const float inv = 1.0f / v;
	/* Pseudodepth^2 at image location */
	//  const float depth2 = powf (0.5f*v*(c_oot + tidx*c_odt), 2.0f);
	const float depth2 = 0.25f*v*v*(c_oot + tidx*c_odt)*(c_oot + tidx*c_odt);
	int i;
	float j, k;
	float img = 0.0f;
	float2 vec1;
	float4 vec2;

	/* Loop over input traces */
	for (i = 0; i < c_obn; i++) {
		vec2 = tex1Dfetch(t_ixy, i*c_ibn + bidx);
		/* vec1.x - squared distance to source from the image point on the surface,
		vec1.y - squared distance to receiver from the image point on the surface */
		vec1.x = vec2.x*vec2.x + vec2.y*vec2.y;
		vec1.y = vec2.z*vec2.z + vec2.w*vec2.w;
		/* Time from source to image point in pseudodepth */
		vec1.x = sqrtf(vec1.x + depth2)*inv;
		/* Time from receiver to image point in pseudodepth */
		vec1.y = sqrtf(vec1.y + depth2)*inv;
		/* double root square time = time to source + time to receiver */
		j = (vec1.x + vec1.y - c_ot)*c_idt; /* Input sample index */
		if (j >= 0.0f && j <= c_maxnt) {
			k = (float)i + 0.5f;
			/* Contribute to the image point */
			img += tex2D(t_i, j + 0.5f, k);
		}
	}

	image[oidx] += img;
}



///////////////////////////////////////////////KTMIG
segy intrace; 	/* input traces */
segy vol;	/* migrated output traces */
int main(int argc, char* argv[]) {
	/* Counters */
	int i = 0, j = 0, k = 0, l, n, iit;

	/* Input data parameters */
	int nt, nx, ny = 1, nix = 1, nin = 1, osize, ntr, btr, dbtr;
	float ot, dt;

	/* Apperture parameters */
	int ix, iy, minix, miniy, maxix, maxiy;
	/* Apperture half-width in each direction */
	int apx, apy;
	/* Apperture first indices in x,y and number of x,y locations */
	int aox, aoy, anx, any;

	/* Image(output) space parameters */
	int ont, onx, ony;
	float oot, oox, ooy;
	float odt, odx, ody;

	/* Antialias filter parameters */
	int maxtri;
	float trfact, trm;

	/* Aperture corners */
	int el_cx1, el_cx2, el_cy1, el_cy2;
	int blk_y;
	float el_x, el_y;

	/* Input traces, output image, velocity */
	float *t, *img, *v, *v_slice;
	/* Aperture indices */
	int *ap;
	/* Coordinates: shot, receiver, and midpoint */

	int star_line, nline, ncdp;
	char *vrmsfile, *imgfile;
	//sf_file data, image, vrms, sxsy, gxgy, cxcy;
	FILE *fp_vfile=NULL;
	FILE *fp_image = NULL;
	initargs(argc, argv);
	requestdoc(0);

	if (!getparint("ncdp", &ncdp)) err("can't get ncdp");
	if (!getparint("ncdp", &nline)) err("can't get nline");
	if (!getparfloat("fm", &odx)) err("can't get odx in celocity");
	if (!getparfloat("fm", &ody)) err("can't get dy in velocity");
	if (!getparfloat("x_beg", &oox)) err("can't get oox in celocity");
	if (!getparfloat("y_beg", &ooy)) err("can't get ooy in velocity");
	if (!getparstring("vfile", &vrmsfile)) { warn("error infile vrm"); }
	if (!getparstring("image", &imgfile)) { warn("error outfile image"); }
	if ((fp_vfile = fopen(vrmsfile, "r")) == NULL)
		err("cannot open infile=%s\n", vrmsfile);
	if ((fp_image = fopen(imgfile, "wb")) == NULL)
		err("cannot open infile=%s\n", imgfile);
	fgettr(fp_vfile, &vol);


	int  aa, diff;

	/* CUDA stuff */
	dim3 dimgrid(1, 1, 1);
	dim3 dimblock(BLOCK_SIZE, 1, 1);
	float *d_t, *d_v, *d_img, val;
	cudaArray *d_rt;
	float2 *d_sxy, *d_gxy, *sxy, *gxy, *cxy;
	float4 *d_ixy;
	int *d_ap;
	int devcnt = 0;
	CUdevice devnum = 0; /* Use the first device by default */
	size_t cudamem = 0;
	cudaDeviceProp deviceProp;




	/* Total time measurement time */
	if (!getparint("aa", &aa)) aa=1;
	/* Antialiaing flag */
	if (!getparint("diff", &diff)) diff = 1;
	/* Differentiation flag */

	cudaSetDevice(0);
	sf_check_gpu_error("Device initialization");
	cudaGetDeviceCount(&devcnt);
	
	if (0 == devcnt)
		err("There is no device supporting CUDA");
#if CUDART_VERSION >= 2020
	n = 0;
	/* Search for a device without kernel exec timeout */
	do {
		cudaGetDeviceProperties(&deviceProp, n);
		sf_check_gpu_error("CUDA device properties request");
		n++;
	} while (n < devcnt && deviceProp.kernelExecTimeoutEnabled);
	if (deviceProp.kernelExecTimeoutEnabled) {
		cudaGetDeviceProperties(&deviceProp, devnum);
		sf_check_gpu_error("CUDA device properties request");
		cudamem = deviceProp.totalGlobalMem;
		warn("Available global memory: %.2fMb", cudamem*1e-6);
		/* Reserve some space for graphics */
		if (cudamem < (1 << 28)) {
			cudamem /= 2;
		}
		else {
			cudamem -= 1 << 28;
		}
		warn("Assuming available global memory: %.2fMb", cudamem*1e-6);
	}
	else { /* Use almost all memory on the available device */
		devnum = n - 1;
		cudamem = deviceProp.totalGlobalMem;
		warn("Available global memory: %.2fMb", cudamem*1e-6);
		cudamem -= 1 << 25;
		warn("Assuming available global memory: %.2fMb", cudamem*1e-6);
	}
#else
	cudaGetDeviceProperties(&deviceProp, devnum);
	sf_check_gpu_error("CUDA device properties request");
	cudamem = deviceProp.totalGlobalMem;
#endif
	cudaSetDevice(devnum);
	sf_check_gpu_error("CUDA device selection");


	gettr(&intrace);
	nt = intrace.ns;
	nx = 6000;
	ny = 1;
	nin = 1;
	nix = 1;
	ntr = nx*ny*nin*nix;
	dt = intrace.dt;
	ot = 0.0;
	ont = nt;
	onx = ncdp;
	ony = nline;
	osize = ont*onx*ony;
	odt = intrace.dt;
	

	oot = 0.;
	
	warn("Image size: %d x %d x %d", ont, onx, ony);

	dbtr = -1;
	/* Desired number of traces per block of threads */
	/* Check memory bounds - there should be enough space for:
	2*osize - velocity volume + image volume,
	5*onx*ony - array of source and receiver coordinates + aperture indices,
	2*nt + 4*onx*ony - for each trace there should be double space for time samples +
	surface vectors to image points */
	btr = (cudamem / sizeof(float) - 2 * osize - 5 * onx*ony) / (2 * nt + 4 * onx*ony);
	if (btr < 1 || btr*nt * sizeof(float) > cudamem)
		err("Not enough memory to migrate this dataset");
	
	warn("Maximum number of traces per block - %d", btr);
	/* If the total number of traces is less than what GPU can digest -
	reduce the block size to the total number of traces */
	if (ntr < btr)
		btr = ntr;
	/* Set number of traces per block to what user requested */
	if (dbtr < btr && dbtr > 0)
		btr = dbtr;
	/* Check if GPU limit on 2D interpolated texture length is not exceeded */
	if (btr > MAX_TEXSIDE)
		btr = MAX_TEXSIDE;
	
	warn("Setting number of traces per block to %d", btr);


	/* File with shot coordinates */
	
	/* File with receiver coordinates */
	
	/* File with midpoint coordinates */
	
	if (!getparint("apx", &apx)) apx = onx / 2;

	/* Apperture half-width in x direction */

	if (!getparint("apy", &apy)) apy = ony / 2;
	/* Apperture half-width in y direction */
	if (!getparint("maxtri", &maxtri)) maxtri = 13;
	/* Maximum half-length of the antialias filter */
	if (!getparfloat("trfact", &trfact)) trfact = 4.0*(0.5*(odx + ody) / dt);
	/* Trace factor for antialias filter length calculation */

	/* Initiate output */
	v_slice = (float*)malloc(ncdp*ont * sizeof(float));
	v= (float*)malloc(osize * sizeof(float));
	img = (float*)malloc(osize * sizeof(float));
	for (i = 0; i < all_line; i++)
	{
		for (j = 0; j < ncdp; j++)
		{
			fgettr(fp_vfile, &vol);
			if (i >= star_line && i < star_line + nline)
			{
				for (iit = 0; iit < nt; iit++)
				{
					v[j*nt + iit] = vol.data[iit];
				}
			}
		}
	}
	
	t = (float*)malloc(btr*nt * sizeof(float)); 
	sxy = (float2*)malloc(btr * sizeof(float2));
	gxy = (float2*)malloc(btr * sizeof(float2));
	cxy = (float2*)malloc(btr * sizeof(float2));
	/* Input data vector on GPU */
	cudaMalloc((void**)&d_t, btr*nt * sizeof(float));
	sf_check_gpu_error("GPU malloc for t");
	/* Input read-only data vector on GPU for texture interpolation */
	cudaMallocArray(&d_rt, &t_i.channelDesc, nt, btr);
	sf_check_gpu_error("GPU malloc for rt");
	cudaBindTextureToArray(t_i, d_rt);
	sf_check_gpu_error("GPU tex bind for rt");
	/* Activate linear interpolation on input traces for
	higher bandwidth in antialiasing */
	t_i.normalized = false;
	t_i.filterMode = cudaFilterModeLinear;
	t_i.addressMode[0] = cudaAddressModeClamp;
	t_i.addressMode[1] = cudaAddressModeClamp;
	/* Array of source coordinates on GPU */
	cudaMalloc((void**)&d_sxy, btr * sizeof(float2));
	sf_check_gpu_error("GPU malloc for sxy");
	cudaBindTexture(0, t_sxy, d_sxy, btr * sizeof(float2));
	sf_check_gpu_error("GPU tex bind for sxy");
	/* Array of receiver coordinates on GPU */
	cudaMalloc((void**)&d_gxy, btr * sizeof(float2));
	sf_check_gpu_error("GPU malloc for gxy");
	cudaBindTexture(0, t_gxy, d_gxy, btr * sizeof(float2));
	sf_check_gpu_error("GPU tex bind for gxy");
	/* Vector of velocities on GPU */
	cudaMalloc((void**)&d_v, osize * sizeof(float));
	sf_check_gpu_error("GPU malloc for v");
	/* Image vector on GPU */
	cudaMalloc((void**)&d_img, osize * sizeof(float));
	sf_check_gpu_error("GPU malloc for img");
	cudaMemset(d_img, 0, osize * sizeof(float));
	sf_check_gpu_error("GPU memset for img");

	/* Array of surface vectors to source/receiver on GPU,
	size is onx*ony rounded to the next number divisible by
	BLOCK_SIZE */
	n = btr*((int)(ceilf(onx*ony / (float)BLOCK_SIZE)*BLOCK_SIZE));
	cudaMalloc((void**)&d_ixy, n * sizeof(float4));
	sf_check_gpu_error("GPU malloc for ixy");
	cudaBindTexture(0, t_ixy, d_ixy, n * sizeof(float4));
	sf_check_gpu_error("GPU tex bind for ixy");

	/* Array of aperture indices */
	n = ((int)(ceilf(onx*ony / (float)BLOCK_SIZE)*BLOCK_SIZE));
	ap = alloc1int(n);
	cudaMalloc((void**)&d_ap, n * sizeof(int));
	sf_check_gpu_error("GPU malloc for ap");
	cudaBindTexture(0, t_ap, d_ap, n * sizeof(int));
	sf_check_gpu_error("GPU tex bind for ap");


	cudaMemcpy(d_v, v, osize * sizeof(float), cudaMemcpyHostToDevice);
	sf_check_gpu_error("Velocities transfer to GPU");

	cudaMemcpyToSymbol("c_nt", &nt, sizeof(int), 0, cudaMemcpyHostToDevice);
	cudaMemcpyToSymbol("c_ont", &ont, sizeof(int), 0, cudaMemcpyHostToDevice);
	cudaMemcpyToSymbol("c_ot", &ot, sizeof(float), 0, cudaMemcpyHostToDevice);
	cudaMemcpyToSymbol("c_oot", &oot, sizeof(float), 0, cudaMemcpyHostToDevice);
	cudaMemcpyToSymbol("c_dt", &dt, sizeof(float), 0, cudaMemcpyHostToDevice);
	val = 1.0 / dt;
	cudaMemcpyToSymbol("c_idt", &val, sizeof(float), 0, cudaMemcpyHostToDevice);
	cudaMemcpyToSymbol("c_odt", &odt, sizeof(float), 0, cudaMemcpyHostToDevice);
	cudaMemcpyToSymbol("c_oox", &oox, sizeof(float), 0, cudaMemcpyHostToDevice);
	cudaMemcpyToSymbol("c_onx", &onx, sizeof(int), 0, cudaMemcpyHostToDevice);
	cudaMemcpyToSymbol("c_odx", &odx, sizeof(float), 0, cudaMemcpyHostToDevice);
	cudaMemcpyToSymbol("c_ooy", &ooy, sizeof(float), 0, cudaMemcpyHostToDevice);
	cudaMemcpyToSymbol("c_ony", &ony, sizeof(int), 0, cudaMemcpyHostToDevice);
	cudaMemcpyToSymbol("c_ody", &ody, sizeof(float), 0, cudaMemcpyHostToDevice);
	cudaMemcpyToSymbol("c_trf", &trfact, sizeof(float), 0, cudaMemcpyHostToDevice);
	trm = maxtri;
	cudaMemcpyToSymbol("c_trm", &trm, sizeof(float), 0, cudaMemcpyHostToDevice);
	val = nt - 1;
	cudaMemcpyToSymbol("c_maxnt", &val, sizeof(float), 0, cudaMemcpyHostToDevice);
	sf_check_gpu_error("Constants transfer to GPU");


	/* Loop over input traces */
	i = 0;
	int kttr = 0;
	do
	{
		
		if (kttr < btr)
		{
			for (i = 0; i < nt; i++)
				t[kttr*nt + i] = intrace.data[i];

			sxy[kttr].x = intrace.sx;
			sxy[kttr].y = intrace.sy;
			gxy[kttr].x = intrace.gx;
			gxy[kttr].y = intrace.gy;
			cxy[kttr].x = (intrace.gx+ intrace.sx)/2;
			cxy[kttr].y = (intrace.gy + intrace.sy) / 2;
			kttr++;
		}
		else
		{
			kttr = 0;
			k = btr;
			/* Find CDP span */
			minix = onx - 1; maxix = 0;
			miniy = ony - 1; maxiy = 0;
			for (l = 0; l < k; l++) {
				ix = (int)((   cxy[l].x - oox) / odx + 0.5f);
				iy = (int)((   cxy[l].y - ooy) / ody + 0.5f);
				if (ix < minix)
					minix = ix;
				if (ix > maxix)
					maxix = ix;
				if (iy < miniy)
					miniy = iy;
				if (iy > maxiy)
					maxiy = iy;
			}
			/* Aperture corners */
			el_cx1 = minix;
			el_cx2 = maxix;
			el_cy1 = miniy;
			el_cy2 = maxiy;
			/* Add apperture width */
			minix -= apx;
			if (minix < 0)
				minix = 0;
			miniy -= apy;
			if (miniy < 0)
				miniy = 0;
			maxix += apx;
			if (maxix >= onx)
				maxix = onx - 1;
			maxiy += apy;
			if (maxiy >= ony)
				maxiy = ony - 1;
			aox = minix;
			aoy = miniy;
			anx = maxix - minix + 1;
			any = maxiy - miniy + 1;
			warn("Rectangular aperture: %d-%d, %d-%d", minix, maxix, miniy, maxiy);

			l = 0;
			for (iy = miniy; iy <= maxiy; iy++) {
				for (ix = minix; ix <= maxix; ix++) {
					blk_y = (iy - miniy)*anx + (ix - minix);
					if (   ( (ix >= el_cx1 && ix <= el_cx2) || (iy >= el_cy1 && iy <= el_cy2) ) && ix>=star_line && ix<= star_line+nline   ) {
						ap[l] = blk_y;
						l++;
						continue;
					}
					/* Distance to corners */
					if (ix < el_cx1)
						el_x = ix - el_cx1;
					else
						el_x = ix - el_cx2;
					if (iy < el_cy1)
						el_y = iy - el_cy1;
					else
						el_y = iy - el_cy2;
					/* Check if the point is within one of the ellipses */
					if ((el_x*el_x / (apx*apx) + el_y*el_y / (apy*apy)) < 1.0f && ix >= star_line && ix <= star_line + nline) {
						ap[l] = blk_y;
						l++;
					}
				}
			}
			/* Send data to GPU */
			cudaMemcpy(d_ap, ap, sizeof(int)*l, cudaMemcpyHostToDevice);
			sf_check_gpu_error("Aperture indices transfer to GPU");

			if (aa)
				cudaMemcpy(d_t, t, sizeof(float)*k*nt, cudaMemcpyHostToDevice);
			else
				cudaMemcpyToArray(d_rt, 0, 0, t, sizeof(float)*k*nt, cudaMemcpyHostToDevice);
			sf_check_gpu_error("Input traces transfer to GPU");
			cudaMemcpy(d_sxy, sxy, k * sizeof(float2), cudaMemcpyHostToDevice);
			sf_check_gpu_error("Input source coordinates transfer to GPU");
			cudaMemcpy(d_gxy, gxy, k * sizeof(float2), cudaMemcpyHostToDevice);
			sf_check_gpu_error("Input receiver coordinates transfer to GPU");
			cudaMemcpyToSymbol("c_obn", &k, sizeof(int), 0, cudaMemcpyHostToDevice);
			cudaMemcpyToSymbol("c_aox", &aox, sizeof(int), 0, cudaMemcpyHostToDevice);
			cudaMemcpyToSymbol("c_aoy", &aoy, sizeof(int), 0, cudaMemcpyHostToDevice);
			cudaMemcpyToSymbol("c_anx", &anx, sizeof(int), 0, cudaMemcpyHostToDevice);
			cudaMemcpyToSymbol("c_any", &any, sizeof(int), 0, cudaMemcpyHostToDevice);
			sf_check_gpu_error("Constants transfer to GPU");

			/* Run the distance calculation kernel */

			n = (int)(ceilf(anx*any / (float)BLOCK_SIZE));
			dimgrid = dim3(n, 1, 1);
			n = (int)(ceilf(anx*any / (float)BLOCK_SIZE)*BLOCK_SIZE);
			cudaMemcpyToSymbol("c_ibn", &n, sizeof(int), 0, cudaMemcpyHostToDevice);
			sf_check_gpu_error("Constants transfer to GPU");

			sf_gpu_ktmig_ixy << <dimgrid, dimblock >> >(d_ixy);
			cudaThreadSynchronize();
			sf_check_gpu_error("Distance kernel invocation");

			
			/* Run antialiasing preparation */
			if (aa || diff) {
				
				dimgrid = dim3(k, 1, 1);
				if (diff) {
					sf_gpu_ktmig_sbdiff << <dimgrid, dimblock >> >(d_t, nt / BLOCK_SIZE);
					cudaThreadSynchronize();
					sf_check_gpu_error("Differentiation kernels invocation");
				}
				if (aa) {
					sf_gpu_ktmig_cint << <dimgrid, dimblock >> >(d_t, nt / BLOCK_SIZE);
					cudaThreadSynchronize();
					sf_gpu_ktmig_acint << <dimgrid, dimblock >> >(d_t, nt / BLOCK_SIZE);
					cudaThreadSynchronize();
					sf_check_gpu_error("Integration kernels invocation");
				}
				cudaMemcpyToArray(d_rt, 0, 0, d_t, sizeof(float)*k*nt, cudaMemcpyDeviceToDevice);
				sf_check_gpu_error("Prerocessed input traces transfer on GPU");

			}

			/* Run the migration kernel */
			n = 0;
			while (l != 0) {
				dimgrid = dim3(ont / BLOCK_SIZE, l < MAX_GRID_SIZE
					? l : MAX_GRID_SIZE, 1);
				if (aa)
					sf_gpu_ktmig_kernel << <dimgrid, dimblock >> >(d_v, d_img, n);
				else
					sf_gpu_ktmig_noaa_kernel << <dimgrid, dimblock >> >(d_v, d_img, n);
				cudaThreadSynchronize();
				sf_check_gpu_error("Migration kernel invocation");
				l = l < MAX_GRID_SIZE ? 0 : l - MAX_GRID_SIZE;
				n += MAX_GRID_SIZE;
			}
			cudaMemcpy(img, d_img, osize * sizeof(float), cudaMemcpyDeviceToHost);
			fseek(fp_image, 0L, SEEK_SET);
			fwrite(img, sizeof(float), osize, fp_image);
		}
	} while (gettr(&intrace));







	  /* Get the image back */
	
	sf_check_gpu_error("Image transfer from GPU");
	cudaUnbindTexture(t_i);
	cudaUnbindTexture(t_sxy);
	cudaUnbindTexture(t_gxy);
	cudaUnbindTexture(t_ixy);
	cudaUnbindTexture(t_ap);
	sf_check_gpu_error("GPU texture unbinding");

	cudaFreeArray(d_rt);
	cudaFree(d_t);
	cudaFree(d_sxy);
	cudaFree(d_gxy);
	cudaFree(d_v);
	cudaFree(d_img);
	cudaFree(d_ixy);
	cudaFree(d_ap);
	sf_check_gpu_error("GPU memory freeing");

	free(ap);
	free(v);
	free(img);
	free(t);
	free(sxy);
	free(gxy);
	free(cxy);



	exit(0);
}
